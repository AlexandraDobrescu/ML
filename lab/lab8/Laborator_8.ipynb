{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laborator 8",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-pub-ro/ML/blob/master/lab/lab8/Laborator_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tmaHAc_KHG_5"
      },
      "source": [
        "# Rețele neurale pentru clasificare imaginilor\n",
        "\n",
        "_Tudor Berariu, 2018_ (tudor.berariu@gmail.com)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKCKTWvsuJuF",
        "colab_type": "text"
      },
      "source": [
        "În cadrul acestui laborator veți implementa o rețea neurală pentru clasificarea imaginilor.\n",
        "Rețeaua va fi compusă din straturi lineare și activări de tip ReLU și un strat softmax înainte de ieșiri. Funcția de cost folosită va fi negative log likelihood. Pentru optimizarea acesteia se va folosi SGD (stochastic gradient descent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgNIWhzoHOIz"
      },
      "source": [
        "## 1. Setul de date MNIST\n",
        "\n",
        "Setul de date MNIST este compus din imagini de 28x28 pixeli reprezentând una dintre cele zece cifre 0-9.\n",
        "\n",
        "Decomentați mai jos comanda `!pip install mnist` pentru a instala pachetul `mnist`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dQiqJyO7E7Ek",
        "colab": {}
      },
      "source": [
        "#!pip install mnist\n",
        "\n",
        "import mnist\n",
        "train_imgs = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_imgs = mnist.test_images()\n",
        "test_labels  = mnist.test_labels()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElGfqnPzuJuO",
        "colab_type": "text"
      },
      "source": [
        "### Exemple din setul de date MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qeftJ_CpE7Eu",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "avJh9wQquJuU",
        "colab": {}
      },
      "source": [
        "idxs = np.random.randint(0, len(train_imgs), 15)\n",
        "imgs = np.concatenate(tuple(train_imgs[idx,:,:] for idx in idxs), axis=1)\n",
        "plt.imshow(imgs)\n",
        "print(\"Labels:\", train_labels[idxs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFP9QnUJuJuY",
        "colab_type": "text"
      },
      "source": [
        "### Standardizarea datelor\n",
        "\n",
        "Datele de intrare (imaginile) vor fi rescalate pentru a avea media zero și deviația standard 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWmt4XVxuJuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean, std  = train_imgs.mean(), train_imgs.std()\n",
        "train_imgs = (train_imgs - mean) / std\n",
        "test_imgs = (test_imgs - mean) / std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZgERUA07IuSr"
      },
      "source": [
        "## 2. Construirea unei rețele de tip feed-forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE-V6sONuJud",
        "colab_type": "text"
      },
      "source": [
        "### Notații\n",
        "  - dimensiunea datelor de intrare este $D = 28 * 28 = 784$, iar dimensiunea ieșirilor rețelei este $K=10$ (numărul de clase)\n",
        "  - rețeaua neurală va avea $L$ straturi\n",
        "  - $B$ va reprezenta dimensiunea batch-ului (numărul de exemple trecute în același timp prin rețea)\n",
        "  - Vom nota cu ${\\bf X} \\in {\\mathbb R}^{B \\times D}$ un batch de intrări $\\left\\lbrace {\\bf x}_0, {\\bf x}_1, \\dots {\\bf x}_B \\right\\rbrace$ și similar ${\\bf Y} \\in {\\mathbb R}^{B \\times K}$\n",
        "  - ${\\bf x}^{(l)}$ reprezintă intrările stratului $l$ (${\\bf x}^{(0)}$ va fi o imagine precum cele din setul MNIST de dimensiune $D$)\n",
        "  - ${\\bf y}^{(l)}$ reprezintă ieșirile stratului $l$ (${\\bf y}^{(L-1)}$ reprezintă ieșirile rețelei)\n",
        "  - ${\\bf \\theta}^{(l)}$ reprezintă parametrii stratului $l$\n",
        "  - ${\\cal L}$ reprezintă funcția de cost (_negative log likelihood_)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YTu4tS8uJue",
        "colab_type": "text"
      },
      "source": [
        "### Straturile rețelei\n",
        "\n",
        "Unele straturi au parametri ce trebuie optimizați în timpul antrenării. Vom nota parametrii stratului $l$ cu $\\bf{\\theta}^{(l)}$.\n",
        "Fiecare strat pe care îl veți implementa va avea trei metode:\n",
        " - `forward` calculează și întoarce ${\\bf y}^{(l)} = f_l\\left({\\bf x}^{(l)}, {\\bf \\theta}^{(l)}\\right)$\n",
        " - `backward` primește $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}$, reține intern $\\frac{\\partial {\\cal L}}{\\partial {\\bf \\theta}^{(l)}}$ și întoarce $\\frac{\\partial {\\cal L}}{\\partial {\\bf x}^{(l)}}$\n",
        " - `update` modifică parametrii locali ${\\bf \\theta}^{(l)}$ folosing gradientul stocat $\\frac{\\partial{\\cal L}}{\\partial{\\bf \\theta}^{(l)}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW206j3euJuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def update(self, *args, **kwargs):\n",
        "        pass  # If a layer has no parameters, then this function does nothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIT6K4IduJuk",
        "colab_type": "text"
      },
      "source": [
        "### Rețeaua neurală\n",
        "\n",
        "  * în faza `forward` ieșirile stratului $l$ devin intrările stratului $l+1$: ${\\bf x}^{(l+1)} = {\\bf y}^{(l)}$\n",
        "  * în faza `backward` gradientul în raport cu intrările stratului $l+1$ devine gradientul în raport cu ieșirile stratului $l$: $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}=\\frac{\\partial {\\cal L}}{\\partial {\\bf x}^{(l+1)}}$\n",
        "  \n",
        "**[Cerința 0]** Completați metoda `backward` din clasa `FeedForwardNetwork`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTn-g3KAuJul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNetwork:\n",
        "    \n",
        "    def __init__(self, layers: List[Layer]):\n",
        "        self.layers = layers\n",
        "        \n",
        "    def forward(self, x: np.ndarray, train: bool = True) -> np.ndarray:\n",
        "        self._inputs = []\n",
        "        for layer in self.layers:\n",
        "            if train:\n",
        "                self._inputs.append(x)\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def backward(self, dy:np.ndarray) -> np.ndarray:\n",
        "        # TODO <0> : Compute the backward phase\n",
        "        raise NotImplementedError\n",
        "        del self._inputs\n",
        "    \n",
        "    def update(self, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            layer.update(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyQLVM4quJup",
        "colab_type": "text"
      },
      "source": [
        "### Stratul linear\n",
        "\n",
        "Un strat linear cu $M$ intrări și $N$ ieșiri are parametrii $\\theta = \\left( {\\bf W}, {\\bf b} \\right)$ unde ${\\bf W} \\in \\mathbb{R}^{M \\times N}$ și ${\\bf b} \\in \\mathbb{R}^{N}$.\n",
        "\n",
        "Pentru un singur exemlu ${\\bf x} \\in {\\mathbb R}^{M}$:\n",
        "$$ {\\bf y} = {\\bf x}^{\\intercal}{\\bf W} + {\\bf b} $$\n",
        "\n",
        "**[Cerința 1]** Implementați metoda `forward` care primește un batch de exemple $X \\in {\\mathbb R}^{B\\times M}$ și întoarce ieșirile corespunzătoare: $Y \\in {\\mathbb R}^{B\\times N}$.\n",
        "\n",
        "**[Cerința 2]** Implementați metoda `backward` care primește un batch de exemple $X \\in {\\mathbb R}^{B\\times M}$ și gradientul în raport cu ieșirile $\\frac{\\partial {\\cal L}}{\\partial {\\bf Y}}$ și realizează două lucruri:\n",
        "  - calculează și salvează intern gradientul $\\frac{\\partial {\\cal L}}{\\partial {\\bf \\theta}}$\n",
        "  - calculează și întoarce gradientul $\\frac{\\partial {\\cal L}}{\\partial {\\bf X}}$\n",
        "  \n",
        "**[BONUS][Cerința 9]** Implementați strategia de optimizare SGD cu _momentum_ (în metoda `update`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S47ZsyKdE7FF",
        "colab": {}
      },
      "source": [
        "class Linear(Layer):\n",
        "    \n",
        "    def __init__(self, insize: int, outsize: int) -> None:\n",
        "        bound = np.sqrt(6. / insize)\n",
        "        self.weight = np.random.uniform(-bound, bound, (insize, outsize))\n",
        "        self.bias = np.zeros((outsize,))\n",
        "        \n",
        "        self.dweight = np.zeros_like(self.weight)\n",
        "        self.dbias = np.zeros_like(self.bias)\n",
        "\n",
        "        \n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        # TODO <1> : compute the output of a linear layer\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        # TODO <2> : compute dweight, dbias and  return dx\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def update(self, mode='SGD', lr=0.001, mu=0.9):\n",
        "        if mode == 'SGD':\n",
        "            self.weight -= lr * self.dweight\n",
        "            self.bias -= lr * self.dbias\n",
        "        elif mode == 'momentum':\n",
        "            # TODO <9>: implement momentum update\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            raise ValueError('mode should be SGD or momentum, not ' + str(mode))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfHlVgDuJut",
        "colab_type": "text"
      },
      "source": [
        "### The Rectified Linear Unit\n",
        "\n",
        "Stratul ReLU aplică următoare următoare transformare neliniară element cu element:\n",
        "$$y = \\max\\left(x, 0\\right)$$\n",
        "\n",
        "**[Cerințele 3-4]** Implementați metodele `forward` și `backward` pentru un strat de activare ReLU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QOR1DJiwE7FJ",
        "colab": {}
      },
      "source": [
        "class ReLU(Layer):\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        # TODO <3> : Compute the output of a rectified linear unit\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        # TODO <4> : Compute the gradient w.r.t. x\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4NrWBTmbI9gW"
      },
      "source": [
        "## 3. Funcția de cost\n",
        "\n",
        "Funcția de cost pe care o vom folosi este _cross entropy_ care combină un _softmax_ și un cost _negative log-likelihood_. (Matematica la tablă)\n",
        "\n",
        "Dacă ${\\bf y}$ reprezintă ieșrile rețelei pentru o intrare ${\\bf x}$, atunci ${\\bf y}$ va avea o dimensiune egală cu numărul de clase $K$. Atunci probabilitatea (prezisă de rețea) ca exemplul ${\\bf x}$ să aparțină clasei $k$ va fi $p_k$:\n",
        "$$\\begin{align}\n",
        "p_k &= \\frac{e^{y_k}}{\\sum_j e^{y_j}} & & \\text{softmax} \\\\\n",
        "{\\cal L} &= -\\log p_t & & \\text{negative log-likelihood}\n",
        "\\end{align}$$\n",
        "\n",
        "\n",
        "Pentru un batch de dimensiune $B$ se va face media costurilor corespunzătare fiecărui exemplu ($p_k$ este o funcție de ${\\bf x}$ și ${\\bf \\theta}$):\n",
        "\n",
        "$$ {\\cal L} = \\frac{1}{B} \\sum_{({\\bf x}, {\\bf t}) \\in Batch} -\\log p_t \\left({\\bf x}, \\theta\\right) $$\n",
        "\n",
        "**[Cerințele 5-6]** Implementați metodele `forward` și `backward` pentru un funcția de cost _cross-entropy_ (o vom privi ca pe un strat suplimentar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDXiDEu8E7FW",
        "colab": {}
      },
      "source": [
        "class CrossEntropy:\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, y: np.ndarray, t: np.ndarray) -> float:\n",
        "        # TODO <5> : Compute the negative log likelihood\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def backward(self, y: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
        "        # TODO <6> : Compute dl/dy\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uz9qM5eHJLNw"
      },
      "source": [
        "### Acuratețea\n",
        "\n",
        "**[Cerința 7]** Calculați acuratețea predicțiilor ${\\bf y}$ în raport cu clasele corecte ${\\bf t}$ (rația exemplelor pentru care clasa corectă a avut probabilitatea prezisă maximă)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3nYfVCBSE7Fe",
        "colab": {}
      },
      "source": [
        "def accuracy(y: np.ndarray, t: np.ndarray) -> float:\n",
        "    # TODO <7> : Compute accuracy\n",
        "    raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mIhtzd2gJQF2"
      },
      "source": [
        "## 4. Antrenarea rețelei neurale\n",
        "\n",
        "**[Cerința 8]** Completați codul de mai jos pentru a calcula gradientul funcției de cost pentru batchul ales și parametrii curenți ai rețelei. _Indiciu_: trebuie să apelați metodele `forward` și `backward` ale rețelei neurale și ale funcției de cost.\n",
        "\n",
        "**[BONUS]** Implementați optimizare cu _momentum_ (vezi TODO-ul numărul 9 din metoda `update`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HTbmZv3YE7Fs",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "HIDDEN_UNITS = 300\n",
        "EPOCHS_NO = 20\n",
        "\n",
        "optimize_args = {'mode': 'SGD', 'lr': .005}\n",
        "\n",
        "net = FeedForwardNetwork([Linear(784, HIDDEN_UNITS),\n",
        "                          ReLU(),\n",
        "                          Linear(HIDDEN_UNITS, 10)])\n",
        "cost_function = CrossEntropy()\n",
        "\n",
        "for epoch in range(EPOCHS_NO):\n",
        "    for b_no, idx in enumerate(range(0, len(train_imgs), BATCH_SIZE)):\n",
        "        # 1. Prepare next batch\n",
        "        x = train_imgs[idx:idx + BATCH_SIZE,:,:].reshape(-1, 784)\n",
        "        t = train_labels[idx:idx + BATCH_SIZE]\n",
        "        \n",
        "        # 2. Compute gradient\n",
        "        \n",
        "        # TODO <8> : Compute gradient\n",
        "        raise NotImplementedError\n",
        "        \n",
        "        # 3. Update network parameters\n",
        "        net.update(**optimize_args)\n",
        "        \n",
        "        print(f'\\rEpoch {epoch + 1:02d} '\n",
        "              f'| Batch {b_no:03d} '\n",
        "              f'| Train NLL: {loss:6.3f} '\n",
        "              f'| Train Acc: {accuracy(y, t) * 100:6.2f}% ', end='')\n",
        "\n",
        "    y = net.forward(test_imgs.reshape(-1, 784), train=False)\n",
        "    test_nll = cost_function.forward(y, test_labels)\n",
        "    print(f'| Test NLL: {test_nll:6.3f} '\n",
        "          f'| Test Acc: {accuracy(y, test_labels) * 100:3.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7zGgHlduJvA",
        "colab_type": "text"
      },
      "source": [
        "## Teste\n",
        "\n",
        "Executați ```test0() and test16() and test7()``` pentru a rula testele."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YaLsPBfuJvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test0():\n",
        "    fakex = [np.random.randn(128, n) for n in [20, 40, 30, 10]]\n",
        "\n",
        "    class DummyLayer:\n",
        "        def __init__(self, idx):\n",
        "            self.idx = idx\n",
        "\n",
        "        def forward(self, x):\n",
        "            return fakex[self.idx + 1]\n",
        "\n",
        "        def backward(self, x, dldy):\n",
        "            if not np.allclose(x, fakex[self.idx]):\n",
        "                raise Exception(\"Intrări greșite în backward\")\n",
        "            if not np.allclose(dldy, -fakex[self.idx+1]):\n",
        "                raise Exception(\"Intrări greșite în backward\")\n",
        "            return -x\n",
        "\n",
        "    try:\n",
        "        net = FeedForwardNetwork([DummyLayer(i) for i in range(3)])\n",
        "        net.forward(fakex[0])\n",
        "        net.backward(-fakex[-1])\n",
        "        print(\"Cerința 0 rezolvată corect!\")\n",
        "        return True\n",
        "    except NotImplementedError as e:\n",
        "        print(\"Cerința 0 nu a fost implementată!\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Cerința 0 are erori.\")\n",
        "        \n",
        "    return False\n",
        "        \n",
        "def test16():\n",
        "    __x = np.array([[-3.0731, -1.9081, -0.7283, -0.0757, -0.7577],\n",
        "                    [ 2.4041, -1.1506, -0.5924,  1.3016,  1.0882],\n",
        "                    [-0.5254,  0.3519, -0.9633, -2.7393, -0.9745]])\n",
        "    __w = np.array([[ 1.3214, -0.5886, -0.0351,  1.2084,  1.2661, -0.9979, -0.1172],\n",
        "                    [-0.4022,  0.1168,  0.9020, -2.0098, -0.5409, -0.3876, -0.1719],\n",
        "                    [-1.1125, -0.5556,  0.8843,  0.6995,  0.4929,  0.7523,  0.1832],\n",
        "                    [ 0.2267,  0.6757,  1.1286, -0.3218,  1.6934, -0.1782, -0.3467],\n",
        "                    [-0.6062,  0.4426,  0.5090,  0.4772, -0.5721,  0.8658, -0.5999]])\n",
        "    __b = np.array([ 0.3335,  0.5051, -0.1393,  1.2116,  1.7836, -0.6597,  0.3553])\n",
        "    __y = np.array([[-1.70746622, 2.10919555, -2.8676804, 0.48630531, -1.1288499, 1.95609904, 1.39083457],\n",
        "                    [4.26749994, 0.64592254, 0.23749513, 6.11524068, 6.73936681, -2.34822291, -0.94127596],\n",
        "                    [0.5391161, -0.89159687, -4.24288533, -0.38789499, -3.62798139, -1.35206921, 1.71422657]])\n",
        "    \n",
        "    __dy = np.array([[ 1.5555, -0.8978, -0.2917, -0.3868, -0.8257, -0.3491, -0.8658],\n",
        "                     [ 1.1146,  1.4914,  0.9591, -0.2613,  0.5887,  0.4794,  0.8565],\n",
        "                     [-0.1552, -1.6319,  1.7642,  1.0503,  0.1035 , -0.7186, -0.9782]])\n",
        "    __dx = np.array([[ 1.53113221,  0.51455541, -2.588423,   -1.49460989, -0.98384103],\n",
        "                     [ 0.41215308,  0.46469672, -0.59552791,  3.04147235, -0.08763244],\n",
        "                     [ 2.92549149, -0.25707023,  2.70531668,  1.15769427,  0.67643021]])\n",
        "    __dw = np.array(\n",
        "        [[-2.01905511,  7.20190418,  2.2752849,   0.00865613,  3.89837344,  2.60289719, 5.23374791],\n",
        "         [-4.30512319, -0.57717827,  0.07387429,  1.40830543,  0.9345816,  -0.13835527, 0.3223155 ],\n",
        "         [-1.64365553,  1.34237165, -2.05517959, -0.57525343,  0.15290988,  0.66248035, 1.0654716 ],\n",
        "         [ 1.75815137,  6.47943337, -3.56222681, -3.18791411,  0.54523986,  2.61887489, 3.85994472],\n",
        "         [ 0.18554777,  3.89349109, -0.45449919, -1.01478565,  1.16539548,  1.48647185, 2.54131586]]\n",
        "    )\n",
        "    __db = np.array([ 2.5149, -1.0383,  2.4316,  0.4022, -0.1335, -0.5883, -0.9875])\n",
        "    \n",
        "    __y_relu = np.array([[0, 2.10919555, 0, 0.48630531, 0, 1.95609904, 1.39083457],\n",
        "                         [4.26749994, 0.64592254, 0.23749513, 6.11524068, 6.73936681, 0, 0],\n",
        "                         [0.5391161, 0, 0, 0, 0, 0, 1.71422657]])\n",
        "    __drelu = np.array([[0, -0.8978, 0, -0.3868, 0, -0.3491, -0.8658],\n",
        "                        [ 1.1146,  1.4914,  0.9591, -0.2613,  0.5887,  0,  0],\n",
        "                        [-0.1552, 0,  0,  0,  0 , 0, -0.9782]])\n",
        "    \n",
        "    __t = np.array([3, 1, 2])\n",
        "    __dl_dy = np.array(\n",
        "        [[ 2.80870645e-03,  1.27661957e-01,  8.80302096e-04, -3.08142112e-01,\n",
        "           5.00952130e-03,  1.09539948e-01,  6.22416775e-02],\n",
        "         [ 1.73238217e-02, -3.32870086e-01,  3.07917841e-04,  1.09927743e-01,\n",
        "           2.05192672e-01,  2.31991342e-05,  9.47329526e-05],\n",
        "         [ 6.60308812e-02,  1.57905168e-02, -3.32780047e-01,  2.61307149e-02,\n",
        "           1.02329216e-03,  9.96358772e-03,  2.13841054e-01]]\n",
        "    )\n",
        "\n",
        "\n",
        "    try:\n",
        "        lin = Linear(5, 7)\n",
        "        lin.weight = __w.copy()\n",
        "        lin.bias = __b.copy()\n",
        "        y = lin.forward(__x.copy())\n",
        "        if not np.allclose(y, __y):\n",
        "            raise Exception(\"Ieșiri greșite\")\n",
        "        print(\"Cerința 1 rezolvată corect!\")\n",
        "    except NotImplementedError as e:\n",
        "        print(\"Cerința 1 nu a fost implementată!\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Cerința 1 are erori.\")\n",
        "        return False\n",
        "        \n",
        "    try:\n",
        "        dx = lin.backward(__x.copy(), __dy.copy())\n",
        "        if not np.allclose(dx, __dx):\n",
        "            raise ValueError(\"dL/dx greșit\")\n",
        "        if not np.allclose(lin.dweight, __dw):\n",
        "            raise ValueError(\"dL/dw greșit\")\n",
        "        if not np.allclose(lin.dbias, __db):\n",
        "            raise ValueError(\"dL/db greșit\")\n",
        "        print(\"Cerința 2 rezolvată corect!\")\n",
        "    except NotImplementedError as e:\n",
        "        print(\"Cerința 2 nu a fost implementată!\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Cerința 2 are erori.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        relu = ReLU()\n",
        "        y_relu = relu.forward(__y.copy())\n",
        "        if not np.allclose(y_relu, __y_relu):\n",
        "            raise ValueError(\"ReLU(x) greșit\")\n",
        "        print(\"Cerința 3 rezolvată corect!\")\n",
        "    except NotImplementedError as e:\n",
        "        print(\"Cerința 3 nu a fost implementată!\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Cerința 3 are erori.\")\n",
        "        return False\n",
        "            \n",
        "    try:\n",
        "        relu = ReLU()\n",
        "        drelu = relu.backward(__y.copy(), __dy.copy())\n",
        "        if not np.allclose(drelu, __drelu):\n",
        "            raise ValueError(\"ReLU.backward greșit\")\n",
        "        print(\"Cerința 4 rezolvată corect!\")\n",
        "    except NotImplementedError as e:\n",
        "        print(\"Cerința 4 nu a fost implementată!\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Cerința 4 are erori.\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        ce = CrossEntropy()\n",
        "        loss = ce.forward(__y.copy(), __t.copy())\n",
        "        if np.abs(loss - 5.1874357237332545) > 1e-6:\n",
        "            raise ValueError(f\"Valoare greșită nll: {loss:f} în loc de 5.1874357237332545\")\n",
        "        print(\"Cerința 5 rezolvată corect!\")\n",
        "    except NotImplementedError as e:\n",
        "        print(\"Cerința 5 nu a fost implementată!\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Cerința 5 are erori.\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        ce = CrossEntropy()\n",
        "        dl_dy = ce.backward(__y.copy(), __t.copy())\n",
        "        if not np.allclose(dl_dy, __dl_dy) > 1e-6:\n",
        "            raise ValueError(f\"Valoare greșită pentru dNLL/dy\")\n",
        "        print(\"Cerința 6 rezolvată corect!\")\n",
        "    except NotImplementedError as e:\n",
        "        print(\"Cerința 6 nu a fost implementată!\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Cerința 6 are erori.\")\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def test7():  # Acuratețea\n",
        "    y = np.array([[ 0.6460014 , -0.05876393, -1.36496105, -0.07057596,  0.54938383],\n",
        "                  [-0.8033942 , -0.51753041,  0.92278036, -1.66303585, -0.36537512],\n",
        "                  [-1.3710599 ,  0.65598193, -0.75527154,  1.21609284,  0.08284123],\n",
        "                  [-1.24696857,  0.32676634,  0.09572539,  1.38316398, -0.14110726],\n",
        "                  [-2.01698315,  2.06123375, -1.68003675,  0.0504592 ,  0.04427597],\n",
        "                  [-0.8893451 ,  1.74695148, -0.29394473,  0.74203068, -0.75185261],\n",
        "                  [ 1.34126333, -0.5272606 ,  1.46458319,  1.59529987,  1.86884676],\n",
        "                  [-0.58987297,  1.10900165, -0.71208103,  0.20478154, -1.26693567],\n",
        "                  [-2.17730677, -1.36147532, -1.49679182,  0.24812177, -0.13368035],\n",
        "                  [-0.48730599,  1.31710647,  0.41765538,  1.19869192, -0.05301611],\n",
        "                  [-0.10655224, -0.21174034,  1.31548647, -0.57990281,  0.85868472],\n",
        "                  [-0.32055613, -2.17817118, -0.28488692,  1.62977524,  0.25150929],\n",
        "                  [ 0.07704727,  1.67710047,  1.83368441, -0.45456845, -0.74474969]])\n",
        "    t = np.array([0, 2, 3, 3, 1, 0, 1, 1, 2, 1, 2, 3, 2])\n",
        "    try:\n",
        "        acc = accuracy(y, t)\n",
        "        if np.abs(acc - 0.7692307692307693) > 1e-7:\n",
        "            raise ValueError(f\"{acc:f} != 10/13\")\n",
        "        print(f\"Cerința 7 rezolvată corect!\")\n",
        "    except NotImplementedError as e:\n",
        "        print(\"Cerința 7 nu a fost implementată!\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"Cerința 7 are erori.\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWQ2e7C4uJvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test0() and test16() and test7()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}