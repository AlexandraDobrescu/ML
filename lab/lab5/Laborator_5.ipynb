{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cs-pub-ro/ML/blob/master/lab/lab5/Laborator_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3a1x3D2pJlE"
   },
   "source": [
    "# Învățare Automată\n",
    "# Clasificare folosind Support Vector Machines\n",
    "### Autori:\n",
    "* Alexandru Sorici - 2017\n",
    "* George Muraru - 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6-C84FKpUfB"
   },
   "source": [
    "## 1. Scopul laboratorului"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTKbkxAwpYhl"
   },
   "source": [
    "Scopul laboratorului îl reprezintă înțelegerea și implementarea unui clasifi-\n",
    "cator SVM, ce poate folosi mai multe tipuri de kernel-uri. În cadrul laboratorului se vor face operații de clasificare pe seturi de date sintetice, ce pun în evidență utilitatea kernel-urilor atunci când spațiul de intrare nu este liniar separabil. Rezolvarea cerințelor se va face pas cu pas urmând indicațiile și explicațiile primite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUwBHNdJp11K"
   },
   "source": [
    "## 2. Descriere teoretică"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zA2cmkJzqh_6"
   },
   "source": [
    "SVM-urile sunt clasificatoare decizionale (i.e. nu au ca rezultat o probabilitate ca un input să facă parte dintr-o clasă dată).\n",
    "\n",
    "O proprietate importantă a SVM-urilor este că determinarea parametrilor\n",
    "modelului se face prin rezolvarea unei probleme de optimizare convexa, ceea\n",
    "ce înseamnă că orice soluție locală este și un optim global.\n",
    "\n",
    "SVM-ul este un clasificator binar, iar problema pe care încearcă să o rezolve este găsirea unui hiperplan de separație care maximizează marginea între hiperplan și cele mai apropiate exemple pozitive și negative.\n",
    "\n",
    "![SVM](https://raw.githubusercontent.com/cs-pub-ro/ML/master/lab/lab4/img/hyperplane-margin.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8i_6oVDI-zp5"
   },
   "source": [
    "## 3. Workspace setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rix9tLXc-2YH"
   },
   "source": [
    "### Dependențe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5TdK5f7-_yR"
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTs6cwy5_Na7"
   },
   "source": [
    "### Câteva bibioteci de care vom avea nevoie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6Y6WMfQ_R5L"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxopt.solvers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "import logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhMSc8oHEdLK"
   },
   "source": [
    "### Parametrii necesari rulării"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09ToJHh-Ef2S"
   },
   "outputs": [],
   "source": [
    "MIN_SUPPORT_VECTOR_MULTIPLIER = 1e-5\n",
    "H = .1  # step size in the mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-tzxK9NOE3v"
   },
   "source": [
    "### Clasa ajutătoare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI5fHhJaOH_Y"
   },
   "outputs": [],
   "source": [
    "class SVMPredictor(object):\n",
    "    def __init__(self,\n",
    "                 kernel,\n",
    "                 bias,\n",
    "                 weights,\n",
    "                 support_vectors,\n",
    "                 support_vector_labels):\n",
    "        self._kernel = kernel\n",
    "        self._bias = bias\n",
    "        self._weights = weights\n",
    "        self._support_vectors = support_vectors\n",
    "        self._support_vector_labels = support_vector_labels\n",
    "        assert len(support_vectors) == len(support_vector_labels)\n",
    "        assert len(weights) == len(support_vector_labels)\n",
    "        logging.info(\"Bias: %s\", self._bias)\n",
    "        logging.info(\"Weights: %s\", self._weights)\n",
    "        logging.info(\"Support vectors: %s\", self._support_vectors)\n",
    "        logging.info(\"Support vector labels: %s\", self._support_vector_labels)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Calculeaza predictia facuta de un SVM, dandu-se inputul x.\n",
    "        Formula de calcul este:\n",
    "            \\sum_m [z_m * y_m * kernel(x_m, x)] + bias\n",
    "\n",
    "            unde m itereaza peste multimea vectorilor de suport\n",
    "        \"\"\"\n",
    "        result = self._bias\n",
    "        for z_i, x_i, y_i in zip(self._weights,\n",
    "                                 self._support_vectors,\n",
    "                                 self._support_vector_labels):\n",
    "            result += z_i * y_i * self._kernel(x_i, x)\n",
    "        return np.sign(result).item()\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        nr_samples, nr_dim = X_test.shape\n",
    "\n",
    "        predictions = np.array(list(map(self.predict, X_test)))\n",
    "        matches = np.multiply(predictions, y_test)\n",
    "\n",
    "        score = sum(matches[matches == 1], 0) * 1.0 / nr_samples\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-G9p2SH-r0V"
   },
   "source": [
    "## Cerințe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGgQrBQJ_wOP"
   },
   "source": [
    "1. Completați codul de mai jos pentru a întoarce 2 funcții:\n",
    "* un kernel liniar\n",
    "$$\n",
    "k(x,y) = \\langle x, y \\rangle\n",
    "$$\n",
    "* un kernel bazat pe Radial Basis Function\n",
    "$$\n",
    "k(x,y) = exp(-\\gamma||x - y||^2)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiCtDFRatFjT"
   },
   "outputs": [],
   "source": [
    "def linear():\n",
    "    \"\"\"\n",
    "    Intoarce o functie anonima ce calculeaza produsul scalar a doi vectori x si y\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def radial_basis(gamma=10):\n",
    "    \"\"\"\n",
    "    Intoarce o functie anonima ce implementeaza forma de Radial Basis Function, avand parametrul \\gamma\n",
    "    :param gamma: parametrul de ponderare a normei diferentei vectorilor x si y\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGPCv94ZD3fw"
   },
   "source": [
    "2. Completați metoda **_gram_matrix** de mai jos astfel încât aceasta să întoarca matricea Gram (K), dandu-se setul de date X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9t9VraJSEIQX"
   },
   "outputs": [],
   "source": [
    "class SVMTrainer(object):\n",
    "    def __init__(self, kernel, c):\n",
    "        self._kernel = kernel\n",
    "        self._c = c\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Dandu-se setul de antrenare X si label-urile y, intoarce un SVM antrenat.\n",
    "        :param X: setul de antrenare avand dimensiunea (num_training_points, num_features)\n",
    "        :param y: eticheta fiecarui input din setup de antrenare, avand dimensiunea (num_training_points, 1)\n",
    "        :return: Predictorul SVM antrenat.\n",
    "        \"\"\"\n",
    "        ## Pas 1 - calculeaza multiplicatorii Lagrange, rezolvand problema duala\n",
    "        lagrange_multipliers = self._compute_multipliers(X, y)\n",
    "\n",
    "        ## Pas 2 - intoarce predictorul SVM pe baza multiplicatorilor Lagrange\n",
    "        return self._construct_predictor(X, y, lagrange_multipliers)\n",
    "\n",
    "    def _gram_matrix(self, X):\n",
    "        \"\"\"\n",
    "        Precalculeaza matricea Gram, folosind kernel-ul dat in constructor, in vederea rezolvarii problemei duale.\n",
    "        :param X: setul de date de antrenare avand dimesiunea (num_samples, num_features)\n",
    "        :return: Matricea Gram precalculata\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "\n",
    "        # TODO: populati matricea Gram conform kernel-ului selectat\n",
    "\n",
    "        return K\n",
    "\n",
    "    def _construct_predictor(self, X, y, lagrange_multipliers):\n",
    "        support_vector_indices = \\\n",
    "            lagrange_multipliers > MIN_SUPPORT_VECTOR_MULTIPLIER\n",
    "\n",
    "        support_multipliers = lagrange_multipliers[support_vector_indices]\n",
    "        support_vectors = X[support_vector_indices]\n",
    "        support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "        # bias = y_k - \\sum z_i y_i  K(x_k, x_i)\n",
    "        ## Desi bias-ul poate fi calculat pe baza unei singure valori din setul vectorilor de suport,\n",
    "        # pentru o forma stabila numeric, folosim o media peste toti vectorii suport\n",
    "        bias = np.mean(\n",
    "            [y_k - SVMPredictor(\n",
    "                kernel=self._kernel,\n",
    "                bias=0.0,\n",
    "                weights=support_multipliers,\n",
    "                support_vectors=support_vectors,\n",
    "                support_vector_labels=support_vector_labels).predict(x_k)\n",
    "             for (y_k, x_k) in zip(support_vector_labels, support_vectors)])\n",
    "\n",
    "        return SVMPredictor(\n",
    "            kernel=self._kernel,\n",
    "            bias=bias,\n",
    "            weights=support_multipliers,\n",
    "            support_vectors=support_vectors,\n",
    "            support_vector_labels=support_vector_labels)\n",
    "\n",
    "    def _compute_multipliers(self, X, y):\n",
    "        \"\"\"\n",
    "        Rezolva problema de optimizare duala, calculand valoarea multiplicatorilor Lagrange\n",
    "        :param X: setul de date de antrenare avand dimensiunea (num_samples, num_features)\n",
    "        :param y: setul de etichete pentru datele de antrenare avand dimensiunea (num_samples, 1)\n",
    "        :return: lista multiplicatorilor Lagrange\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        K = self._gram_matrix(X)\n",
    "        \"\"\"\n",
    "        Metoda din cvxopt ce rezolva o problema de optimizare patratica are urmatoarea formulare\n",
    "        min 1/2 x^T P x + q^T x\n",
    "        a.i.\n",
    "        Gx < h\n",
    "        Ax = b\n",
    "        \n",
    "        unde x este vectorul de valori x_i, de dimensiune (n_samples, 1) a carui valoare se cauta\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Problema duala pentru SVM cere:\n",
    "        min 1/2 a^T Q a - 1^T a\n",
    "        a.i.\n",
    "        0 <= a_i, orice i\n",
    "        a_i <= C, orice i\n",
    "        y^T a = 0\n",
    "\n",
    "        unde Q = (y * y^T) . K (i.e. inmultire matriceala intre y si y^T si apoi inmultire element-cu-element cu matricea K)\n",
    "\n",
    "        Aici vectorul pe care il cautam este `a' (cel al multiplicatorilor Lagrange) de dimensiune (n_samples, 1).\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Cerinta este de a gasi maparea corecta intre forma duala pentru SVM si cea utilizata de cvxopt, i.e. ce valori trebuie sa ia\n",
    "        matricile P, G si A si vectorii q, h si b, astfel incat ei sa reprezinte expresiile din forma duala SVM.\n",
    "        \n",
    "        Vectorul `a' tine loc de `x' in forma ecuatiilor pentru cvxopt.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO calculeaza valoarea matricii P = (y * y^T) . K \n",
    "        # P = cvxopt.matrix(...)\n",
    "\n",
    "        # TODO calculeaza valoarea vectorului q\n",
    "        # q = cvxopt.matrix(...)\n",
    "\n",
    "        # setam G si h in doi pasi a.i sa cuprinda cele doua inegalitati, 0 <= a_i si a_i <= C\n",
    "        # TODO seteaza G_std si h_std pentru a rezolva -a <= 0\n",
    "        # G_std = cvxopt.matrix(...)\n",
    "        # h_std = cvxopt.matrix(...)\n",
    "\n",
    "        # TODO seteaza G_slack si h_slack pentru a rezolva a <= C\n",
    "        # G_slack = cvxopt.matrix(...)\n",
    "        # h_slack = cvxopt.matrix(...)\n",
    "\n",
    "        # TODO obtine G si h prin suprapunere a variabilelor anterioare (vezi functia numpy.vstack)\n",
    "        # G = cvxopt.matrix(...)\n",
    "        # h = cvxopt.matrix(...)\n",
    "\n",
    "        # TODO seteaza A si b a.i. sa acopere relatia y^T a = \\sum y_i a_i\n",
    "        # A = cvxopt.matrix(..., tc = 'd')\n",
    "        # b = cvxopt.matrix(...)\n",
    "\n",
    "        cvxopt.solvers.options['show_progress'] = False\n",
    "        # solution = cvxopt.solvers.qp(P, q, G, h, A, b)        # decomentati linia cand ati implementat matricile de mai sus\n",
    "\n",
    "        # intoarcem multiplicatorii Lagrange sub forma liniarizata - vezi functia np.ravel\n",
    "        # return np.ravel(solution['x'])                        # decomentati linia cand ati implementat matricile de mai sus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKiqXjYcF8YU"
   },
   "source": [
    "3. Completați în funcția **compute_multipliers** (în clasa de mai sus), astfel încât aceasta să întoarcă lista multiplicatorilor Lagrange ce rezolvă formularea\n",
    "duală a problemei de optimizare SVM descrisă în pdf.\n",
    "\n",
    "   Biblioteca *cvxopt* dispune de o metodă de rezolvare a problemelor de optimizare pătratică ce au următoarea formă:\n",
    "$$\n",
    "\\min\\frac{1}{2}x^TPx + q^Tx\\\\\n",
    "a.î. \\\\ \n",
    "Gx \\prec h \\\\\n",
    "Ax = b\n",
    "$$\n",
    "\n",
    "    Problema duală pentru SVM poate fi rescrisă sub forma matricială astfel:\n",
    "$$\n",
    "\\min\\frac{1}{2}a^TQa - 1^Ta\\\\\n",
    "a.î.\\\\\n",
    "0 \\leq a_i, i = 1,...,n\\\\\n",
    "a_i \\leq C, i = 1,...,n\\\\\n",
    "y^Ta = 0\n",
    "$$\n",
    "\n",
    "    unde:\n",
    "    * $Q = (yy^T)\\cdot K$ (înmulțire matriceală între $y$ și $y^T$ și apoi înmulțire element-cu-element cu matricea K)\n",
    "\n",
    "    HINT: Folosiți funcțiile *cvxopt.matrix, numpy.outer, numpy.ones, numpy.zeros, numpy.diag, numpy.vstack*. Folosiți peste tot unde creați matrici sau vectori, funcția *cvxopt.matrix* ca wrapper peste expresii de *numpy*. Wrapper-ul este necesar apelării ulterioare a solver-ului din biblioteca *cvxopt*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrBVHr1nLG6Z"
   },
   "source": [
    "4. Analizați modul în care se modifică scorul de clasificare și forma curbei de\n",
    "separație atunci când variați parametrul C in intervalul $[10^{-3},10^3]$ prin puteri ale lui 10.\n",
    "\n",
    "   Implementați o funcție **num_misclassified** în clasa **SVMTrainer** care să întoarcă numărul de vectori de intrare clasificați incorect. Luați drept punct de plecare codul din funcția **score** din clasa **SVMPredictor**. Observati influența parametrului C asupra rezultatului întors de funcția **num_misclassified**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6CoJAktMV_I"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "lDueFUXSMUwL"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "num_samples = 100\n",
    "num_features = 2\n",
    "H = .1\n",
    "\n",
    "X = np.matrix(np.random.normal(size=num_samples * num_features)\n",
    "                        .reshape(num_samples, num_features))\n",
    "    y = np.ravel(2 * (X.sum(axis=1) > 0) - 1.0)\n",
    "    linearly_separable = (X, y)\n",
    "\n",
    "    names = [\"SVM Linear c = 0.1\", \"SVM RBF c = 0.1\", \"SVM Linear c = 1000\", \"SVM RBF c = 1000\"]\n",
    "    datasets = [make_moons(noise=0.1, random_state=0),\n",
    "                make_circles(noise=0.1, factor=0.5, random_state=1),\n",
    "                linearly_separable\n",
    "                ]\n",
    "\n",
    "    classifiers = [\n",
    "        svm.SVMTrainer(kernel=linear(), c = 0.1),\n",
    "        svm.SVMTrainer(kernel=radial_basis(gamma=1), c = 0.1),\n",
    "        svm.SVMTrainer(kernel=linear(), c=1000),\n",
    "        svm.SVMTrainer(kernel=radial_basis(gamma=1), c=1000)\n",
    "    ]\n",
    "\n",
    "    figure = plt.figure(figsize=(27, 9))\n",
    "    i = 1\n",
    "    for ds_cnt, ds in enumerate(datasets):\n",
    "        X, y = ds\n",
    "        ## modify labels to correspond to SVM categories {-1, 1}\n",
    "        y[y == 0] = -1\n",
    "\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=.34, random_state=42)\n",
    "\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, H),\n",
    "                             np.arange(y_min, y_max, H))\n",
    "\n",
    "        # just plot the dataset first\n",
    "        cm = plt.cm.RdBu\n",
    "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(\"Input data\")\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        i += 1\n",
    "\n",
    "        # iterate over classifiers\n",
    "        for name, clf in zip(names, classifiers):\n",
    "            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "            \n",
    "            predictor = clf.fit(X_train, y_train)\n",
    "            score = predictor.score(X_test, y_test)\n",
    "\n",
    "            mesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "            #print mesh\n",
    "\n",
    "            start_ts = datetime.datetime.now()\n",
    "            Z = np.array(map(predictor.predict, mesh))\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            end_ts = datetime.datetime.now()\n",
    "            diff = end_ts - start_ts\n",
    "            print (\"SVM mesh for dataset %s and classifier %s --> %s s.\" % (ds_cnt, name, diff.seconds))\n",
    "\n",
    "\n",
    "            ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "            # Plot also the training points\n",
    "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker = '+', cmap=cm_bright)\n",
    "            # and testing points\n",
    "            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker = '+', cmap=cm_bright, alpha=0.6)\n",
    "\n",
    "            # Plot the support vectors\n",
    "            ax.scatter(predictor._support_vectors[:, 0], predictor._support_vectors[:, 1], marker = 'v', c=predictor._support_vector_labels, cmap=cm_bright)\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "            if ds_cnt == 0:\n",
    "                ax.set_title(name)\n",
    "            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                    size=15, horizontalalignment='right')\n",
    "            i += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNn7A0noGBNa+cyDeN309Wg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Laborator 5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
